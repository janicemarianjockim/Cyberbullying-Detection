{"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"linear-svm-classification-of-sentiment-in-tweets.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# Before we begin, we supress deprecation warnings resulting from nltk on Kaggle\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "],"metadata":{"_cell_guid":"3fb90d46-4cf7-edf1-f149-6fdd49ecabb2","id":"RZZvkIJcBqSf","executionInfo":{"status":"ok","timestamp":1650274634142,"user_tz":-330,"elapsed":1447,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"metadata":{"id":"50p9NBseBsR4","executionInfo":{"status":"ok","timestamp":1650274666322,"user_tz":-330,"elapsed":28986,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google import colab\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/Colab Notebooks/FYP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"orD19FShBzun","executionInfo":{"status":"ok","timestamp":1650274703109,"user_tz":-330,"elapsed":17871,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"f759575c-2f22-408d-ca7e-24b8d3ad66bc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive/My Drive/Colab Notebooks/FYP\n"]}]},{"cell_type":"markdown","source":["# Linear SVM classification of sentiment in tweets about airlines\n","\n","This notebook describes an attempt to classify tweets by sentiment. It describes the initial data exploration, as well as implementation of a linear one-vs-rest Support-Vector-Machine (SVM) classifier."],"metadata":{"_cell_guid":"2595fd17-1db0-1448-d3c9-4bfd1bc33644","id":"PQJNzRyaBqSp"}},{"cell_type":"markdown","source":["## What is in the dataset?\n","\n","It's always good to start by exploring the data that we have available. To do this we load the raw csv file using [Pandas][1] and check what the columns are.\n","\n","  [1]: http://pandas.pydata.org/"],"metadata":{"_cell_guid":"1302dbd6-79d2-0a40-ddeb-7231a2f8e3e8","id":"6xxZ4GI4BqSr"}},{"cell_type":"code","source":["import pandas as pd\n","tweets = pd.read_csv(\"cyberbullying_tweets.csv\")\n","list(tweets.columns.values)"],"metadata":{"_cell_guid":"f5f1a8d6-2465-1f81-50ca-173e7d15d79c","colab":{"base_uri":"https://localhost:8080/"},"id":"O6ehCXjrBqSs","executionInfo":{"status":"ok","timestamp":1650274724533,"user_tz":-330,"elapsed":2134,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"9e0cb6cf-cd63-4191-98e5-718af4460003"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['TWEET_TEXT',\n"," 'CYBERBULLYING_TYPE',\n"," 'AGE',\n"," 'GENDER',\n"," 'RELIGION',\n"," 'ETHNICITY',\n"," 'OTHER',\n"," 'RESULT']"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["We want to be able to determine the sentiment of a tweet without any other information but the tweet text itself, hence the 'text' column is our focus. Using the text we are going to try and predict 'airline_sentiment'.\n","\n","First we take a look at what a typical record looks like."],"metadata":{"_cell_guid":"39182328-e7fa-1a61-723b-3cbf2517751c","id":"OdaEPAyoBqSu"}},{"cell_type":"code","source":["tweets.head()"],"metadata":{"_cell_guid":"56171d3a-313c-a946-fb1e-8c86cc3d1966","colab":{"base_uri":"https://localhost:8080/","height":206},"id":"N4-qtYspBqSv","executionInfo":{"status":"ok","timestamp":1650274731527,"user_tz":-330,"elapsed":548,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"d901c199-128c-45fc-88cf-538bf8f56ab7"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          TWEET_TEXT CYBERBULLYING_TYPE  AGE  \\\n","0  In other words #katandandre, your food was cra...  not_cyberbullying    0   \n","1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying    0   \n","2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying    0   \n","3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying    0   \n","4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying    0   \n","\n","   GENDER  RELIGION  ETHNICITY  OTHER  RESULT  \n","0       0         0          0      0       0  \n","1       0         0          0      0       0  \n","2       0         0          0      0       0  \n","3       0         0          0      0       0  \n","4       0         0          0      0       0  "],"text/html":["\n","  <div id=\"df-2c05b83f-ee2a-4440-8b20-c420a8013b05\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TWEET_TEXT</th>\n","      <th>CYBERBULLYING_TYPE</th>\n","      <th>AGE</th>\n","      <th>GENDER</th>\n","      <th>RELIGION</th>\n","      <th>ETHNICITY</th>\n","      <th>OTHER</th>\n","      <th>RESULT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>In other words #katandandre, your food was cra...</td>\n","      <td>not_cyberbullying</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n","      <td>not_cyberbullying</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n","      <td>not_cyberbullying</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n","      <td>not_cyberbullying</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n","      <td>not_cyberbullying</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c05b83f-ee2a-4440-8b20-c420a8013b05')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2c05b83f-ee2a-4440-8b20-c420a8013b05 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2c05b83f-ee2a-4440-8b20-c420a8013b05');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Now lets take a look at what sentiments have been found."],"metadata":{"_cell_guid":"cb5ee880-d138-4e49-7f09-e902a5318785","id":"szuvur_hBqSx"}},{"cell_type":"markdown","source":["It turns out that our dataset is unbalanced with significantly more negative than positive tweets. We will focus on the issue of identifying negative tweets, and hence treat neutral and positive as one class. It's good to keep in mind that, while a terrible classifier, if we always guessed a tweet was negative we'd be right 62.7% of the time (9178 of 14640). That clearly wouldn't be a very useful classifier, but worth to remember."],"metadata":{"_cell_guid":"92085794-bb90-cc78-1e96-f9e7f3f11f84","id":"sY7To1fjBqS3"}},{"cell_type":"markdown","source":["# What characterizes text of different sentiments?\n","\n","While we still haven't decided what classification method to use, it's useful to get an idea of how the different texts look. This might be an \"old school\" approach in the age of deep learning, but lets indulge ourselves nevertheless. \n","\n","To explore the data we apply some crude preprocessing. We will tokenize and lemmatize using [Python NLTK][1], and transform to lower case. As words mostly matter in context we'll look at bi-grams instead of just individual tokens.\n","\n","As a way to simplify later inspection of results we will store all processing of data together with it's original form. This means we will extend the Pandas dataframe into which we imported the raw data with new columns as we go along.\n","\n","### Preprocessing\n","Note that we remove the first two tokens as they always contain \"@ airline_name\". We begin by defining our normalization function.\n","\n","\n","  [1]: http://www.nltk.org/"],"metadata":{"_cell_guid":"e4530f8b-ce34-a6c5-21e1-85db7e173211","id":"e4Qyh-JWBqS6"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRqLrT2_Cn2s","executionInfo":{"status":"ok","timestamp":1650274768527,"user_tz":-330,"elapsed":533,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"00876ebd-20a5-45a1-b08f-5638a53a0975"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import re, nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","def normalizer(tweet):\n","    only_letters = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n","    tokens = nltk.word_tokenize(only_letters)[2:]\n","    lower_case = [l.lower() for l in tokens]\n","    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n","    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n","    return lemmas"],"metadata":{"_cell_guid":"03b28e2b-38d0-b478-856d-25ac7f052dff","id":"jA7YkjS5BqS9","executionInfo":{"status":"ok","timestamp":1650274771986,"user_tz":-330,"elapsed":304,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phMkHe-0Ctbf","executionInfo":{"status":"ok","timestamp":1650274792857,"user_tz":-330,"elapsed":1124,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"5970d2a9-151a-4932-b347-58cae2f53284"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTZn_horCyHn","executionInfo":{"status":"ok","timestamp":1650274811587,"user_tz":-330,"elapsed":830,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"f5e103cc-2d84-43a1-f124-e9e36cbb84be"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["normalizer(\"Here is text about an airline I like.\")"],"metadata":{"_cell_guid":"e14cfc54-490d-ce20-4031-cd385a6999d0","colab":{"base_uri":"https://localhost:8080/"},"id":"dggoCA2iBqS_","executionInfo":{"status":"ok","timestamp":1650274815374,"user_tz":-330,"elapsed":2611,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"6d1f0f75-d37c-4f56-e3ff-a12ffaf3fb72"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['text', 'airline', 'like']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["pd.set_option('display.max_colwidth', -1) # Setting this so we can see the full content of cells\n","tweets['normalized_tweet'] = tweets.TWEET_TEXT.apply(normalizer)\n","tweets[['TWEET_TEXT','normalized_tweet']].head()"],"metadata":{"_cell_guid":"33e1be47-7c09-948b-101d-380ccaeb4196","colab":{"base_uri":"https://localhost:8080/","height":322},"id":"bGHhQQU_BqTB","executionInfo":{"status":"ok","timestamp":1650274913093,"user_tz":-330,"elapsed":16337,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"70c0fcbf-1e52-4e99-f42d-ef937b14f451"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                                                                            TWEET_TEXT  \\\n","0  In other words #katandandre, your food was crapilicious! #mkr                                                         \n","1  Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc   \n","2  @XochitlSuckkks a classy whore? Or more red velvet cupcakes?                                                          \n","3  @Jason_Gio meh. :P  thanks for the heads up, but not too concerned about another angry dude on twitter.               \n","4  @RudhoeEnglish This is an ISIS account pretending to be a Kurdish account.  Like Islam, it is all lies.               \n","\n","                                                                                          normalized_tweet  \n","0  [word, katandandre, food, crapilicious, mkr]                                                             \n","1  [aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio, neighbour, wonderlandten, etc]  \n","2  [classy, whore, red, velvet, cupcake]                                                                    \n","3  [meh, p, thanks, head, concerned, another, angry, dude, twitter]                                         \n","4  [isi, account, pretending, kurdish, account, like, islam, lie]                                           "],"text/html":["\n","  <div id=\"df-4bfbe704-9681-4640-b130-c4502fdf5de9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TWEET_TEXT</th>\n","      <th>normalized_tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>In other words #katandandre, your food was crapilicious! #mkr</td>\n","      <td>[word, katandandre, food, crapilicious, mkr]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc</td>\n","      <td>[aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio, neighbour, wonderlandten, etc]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@XochitlSuckkks a classy whore? Or more red velvet cupcakes?</td>\n","      <td>[classy, whore, red, velvet, cupcake]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@Jason_Gio meh. :P  thanks for the heads up, but not too concerned about another angry dude on twitter.</td>\n","      <td>[meh, p, thanks, head, concerned, another, angry, dude, twitter]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@RudhoeEnglish This is an ISIS account pretending to be a Kurdish account.  Like Islam, it is all lies.</td>\n","      <td>[isi, account, pretending, kurdish, account, like, islam, lie]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bfbe704-9681-4640-b130-c4502fdf5de9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4bfbe704-9681-4640-b130-c4502fdf5de9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4bfbe704-9681-4640-b130-c4502fdf5de9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from nltk import ngrams\n","def ngrams(input_list):\n","    #onegrams = input_list\n","    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n","    trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n","    return bigrams+trigrams\n","tweets['grams'] = tweets.normalized_tweet.apply(ngrams)\n","tweets[['grams']].head()"],"metadata":{"_cell_guid":"5e83d39c-c35d-dc3f-1d24-013810053bc9","colab":{"base_uri":"https://localhost:8080/","height":319},"id":"A_-NrQn6BqTC","executionInfo":{"status":"ok","timestamp":1650274919801,"user_tz":-330,"elapsed":853,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"e4f7bca1-88b3-4db8-9c17-8838722a98c9"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                                                                                                                                                                                                                                                                                                                                                                  grams\n","0  [word katandandre, katandandre food, food crapilicious, crapilicious mkr, word katandandre food, katandandre food crapilicious, food crapilicious mkr]                                                                                                                                                                                                                                                                              \n","1  [aussietv white, white mkr, mkr theblock, theblock imacelebrityau, imacelebrityau today, today sunrise, sunrise studio, studio neighbour, neighbour wonderlandten, wonderlandten etc, aussietv white mkr, white mkr theblock, mkr theblock imacelebrityau, theblock imacelebrityau today, imacelebrityau today sunrise, today sunrise studio, sunrise studio neighbour, studio neighbour wonderlandten, neighbour wonderlandten etc]\n","2  [classy whore, whore red, red velvet, velvet cupcake, classy whore red, whore red velvet, red velvet cupcake]                                                                                                                                                                                                                                                                                                                       \n","3  [meh p, p thanks, thanks head, head concerned, concerned another, another angry, angry dude, dude twitter, meh p thanks, p thanks head, thanks head concerned, head concerned another, concerned another angry, another angry dude, angry dude twitter]                                                                                                                                                                             \n","4  [isi account, account pretending, pretending kurdish, kurdish account, account like, like islam, islam lie, isi account pretending, account pretending kurdish, pretending kurdish account, kurdish account like, account like islam, like islam lie]                                                                                                                                                                               "],"text/html":["\n","  <div id=\"df-21e880dd-6ed3-435e-98f5-b7bd424e87c4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grams</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[word katandandre, katandandre food, food crapilicious, crapilicious mkr, word katandandre food, katandandre food crapilicious, food crapilicious mkr]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[aussietv white, white mkr, mkr theblock, theblock imacelebrityau, imacelebrityau today, today sunrise, sunrise studio, studio neighbour, neighbour wonderlandten, wonderlandten etc, aussietv white mkr, white mkr theblock, mkr theblock imacelebrityau, theblock imacelebrityau today, imacelebrityau today sunrise, today sunrise studio, sunrise studio neighbour, studio neighbour wonderlandten, neighbour wonderlandten etc]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[classy whore, whore red, red velvet, velvet cupcake, classy whore red, whore red velvet, red velvet cupcake]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[meh p, p thanks, thanks head, head concerned, concerned another, another angry, angry dude, dude twitter, meh p thanks, p thanks head, thanks head concerned, head concerned another, concerned another angry, another angry dude, angry dude twitter]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[isi account, account pretending, pretending kurdish, kurdish account, account like, like islam, islam lie, isi account pretending, account pretending kurdish, pretending kurdish account, kurdish account like, account like islam, like islam lie]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21e880dd-6ed3-435e-98f5-b7bd424e87c4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-21e880dd-6ed3-435e-98f5-b7bd424e87c4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-21e880dd-6ed3-435e-98f5-b7bd424e87c4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["And now some counting."],"metadata":{"_cell_guid":"c0b51587-a300-8f90-cd30-f44791540f96","id":"SHkJYjdIBqTD"}},{"cell_type":"code","source":["import collections\n","def count_words(input):\n","    cnt = collections.Counter()\n","    for row in input:\n","        for word in row:\n","            cnt[word] += 1\n","    return cnt"],"metadata":{"_cell_guid":"3a4c4888-2be9-54d6-9b5b-9b8b336cdfa8","id":"m8qnabkKBqTE","executionInfo":{"status":"ok","timestamp":1650274924908,"user_tz":-330,"elapsed":394,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["tweets[(tweets.RESULT == 1)][['grams']].apply(count_words)['grams'].most_common(20)"],"metadata":{"_cell_guid":"c0395d2b-61cb-213e-6c38-e476770a9028","colab":{"base_uri":"https://localhost:8080/"},"id":"krSbTBH0BqTF","executionInfo":{"status":"ok","timestamp":1650275006144,"user_tz":-330,"elapsed":1925,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"d5889dde-016b-4964-dbcf-b4156f799628"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('high school', 4561),\n"," ('rape joke', 2151),\n"," ('http co', 1911),\n"," ('dumb as', 1505),\n"," ('as nigger', 1326),\n"," ('bullied high', 1324),\n"," ('bullied high school', 1317),\n"," ('dumb as nigger', 1193),\n"," ('school bully', 1165),\n"," ('dumb nigger', 1136),\n"," ('girl bullied', 1093),\n"," ('dumb fuck', 1014),\n"," ('obama dumb', 981),\n"," ('obama dumb as', 965),\n"," ('islamic terrorism', 863),\n"," ('christian woman', 809),\n"," ('gay joke', 770),\n"," ('fuck obama', 750),\n"," ('fuck obama dumb', 739),\n"," ('tayyoung fuck', 638)]"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["We can already tell there's a pattern here. Sentences like \"cancelled flight\", \"late flight\", \"booking problems\",  \"delayed flight\" stand out clearly. Lets check the positive tweets."],"metadata":{"_cell_guid":"cdca1e1c-eb02-239f-eb98-dc16e32dbcde","id":"fN_bFShuBqTG"}},{"cell_type":"code","source":["tweets[(tweets.RESULT == 0)][['grams']].apply(count_words)['grams'].most_common(20)"],"metadata":{"_cell_guid":"aa00dfb3-81e0-980b-cfd2-e56454a612c4","colab":{"base_uri":"https://localhost:8080/"},"id":"ZYM_rLqOBqTH","executionInfo":{"status":"ok","timestamp":1650275023168,"user_tz":-330,"elapsed":560,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"975b60e1-f64f-48ac-d853-15cfbf5bf15d"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('http co', 960),\n"," ('kat andre', 95),\n"," ('mkr mkr', 75),\n"," ('gt gt', 69),\n"," ('mkr http', 57),\n"," ('mkr http co', 56),\n"," ('bullying http', 47),\n"," ('gon na', 46),\n"," ('instant restaurant', 46),\n"," ('gt gt gt', 46),\n"," ('amp andre', 41),\n"," ('kat amp', 39),\n"," ('kat amp andre', 37),\n"," ('bullying se', 35),\n"," ('bullying http co', 31),\n"," ('de bullying', 30),\n"," ('bit ly', 29),\n"," ('high school', 29),\n"," ('http bit', 28),\n"," ('http bit ly', 28)]"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["Some more good looking patterns here. We can however see that with 3-grams clear patterns are rare. \"great customer service\" occurs 12 times in 2362 positive responses, which really doesn't say much in general. \n","\n","Satisfied that our data looks possible to work with begin to construct our first classifier.\n","\n","# Linear SVM classifier\n","We will build a simple, linear Support-Vector-Machine (SVM) classifier. The classifier will take into account each unique word present in the sentence, as well as all consecutive words. To make this representation useful for our SVM classifier we transform each sentence into a vector. The vector is of the same length as our vocabulary, i.e. the list of all words observed in our training data, with each word representing an entry in the vector. If a particular word is present, that entry in the vector is 1, otherwise 0.\n","\n","To create these vectors we use the CountVectorizer from [sklearn][1]. \n","\n","\n","  [1]: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"],"metadata":{"_cell_guid":"e11015ce-3f91-7708-36ab-1ccad7143cfb","id":"4a-jichUBqTH"}},{"cell_type":"markdown","source":["## Preparing the data"],"metadata":{"_cell_guid":"13b5d690-eb25-6243-7a40-c06cfa3bf825","id":"DXUHRRkiBqTI"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.sparse import hstack\n","from sklearn.feature_extraction.text import CountVectorizer\n","count_vectorizer = CountVectorizer(ngram_range=(1,2))"],"metadata":{"_cell_guid":"f16bfa38-fb30-aff1-d53a-54d23b6a4322","id":"cpKth_4DBqTJ","executionInfo":{"status":"ok","timestamp":1650275028956,"user_tz":-330,"elapsed":311,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["vectorized_data = count_vectorizer.fit_transform(tweets.TWEET_TEXT)\n","indexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))"],"metadata":{"_cell_guid":"3a964c4c-c16e-6592-e68e-3f6ef1197fdc","id":"J7YVJ7IXBqTK","executionInfo":{"status":"ok","timestamp":1650275048443,"user_tz":-330,"elapsed":5643,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["targets = tweets.RESULT"],"metadata":{"_cell_guid":"457bdfd2-685d-bbc1-699f-1954108dd1d5","id":"FNrVr0WwBqTL","executionInfo":{"status":"ok","timestamp":1650275463843,"user_tz":-330,"elapsed":313,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["To check performance of our classifier we want to split our data in to train and test."],"metadata":{"_cell_guid":"f215daf3-d2b9-134b-a371-98ef9bc3c4e7","id":"G33DRPIBBqTM"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","data_train, data_test, targets_train, targets_test = train_test_split(indexed_data, targets, test_size=0.4, random_state=0)\n","data_train_index = data_train[:,0]\n","data_train = data_train[:,1:]\n","data_test_index = data_test[:,0]\n","data_test = data_test[:,1:]"],"metadata":{"_cell_guid":"c99298bd-84e9-d31e-58d1-00dbbdfa83e1","id":"MMKQFccIBqTM","executionInfo":{"status":"ok","timestamp":1650275468583,"user_tz":-330,"elapsed":466,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["## Fitting the classifier\n","\n","We're now ready to fit the classifier to our data. We'll spend more time on hyper parameter tuning later, so for now we just pick some reasonable guesses. Note here that we use the OneVsRestClassifier. This allows us to get the probability distribution over all three classes. Behind the scenes we actually create three classifiers. Each of these classifiers determines the probability that the datapoint belongs to it's corresponding class, or any of the other classes. Hence the name OneVsRest."],"metadata":{"_cell_guid":"936edaf6-9b95-44f7-fc0f-80f0699b66aa","id":"ocGXBYnZBqTN"}},{"cell_type":"code","source":["from sklearn import svm\n","from sklearn.multiclass import OneVsRestClassifier\n","clf = OneVsRestClassifier(svm.SVC(gamma=0.01, C=100., probability=True, class_weight='balanced', kernel='linear'))\n","clf_output = clf.fit(data_train, targets_train)"],"metadata":{"_cell_guid":"ed3e70df-a983-fb02-1b47-0499bda60850","id":"QE2CnLfbBqTN","executionInfo":{"status":"ok","timestamp":1650276590438,"user_tz":-330,"elapsed":1117421,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation of results"],"metadata":{"_cell_guid":"3f5a37b6-e56f-036b-e44e-9ec4814cd8d7","id":"2P-bHhyUBqTO"}},{"cell_type":"code","source":["clf.score(data_test, targets_test)"],"metadata":{"_cell_guid":"1af23c8b-7c12-63f5-260b-9f34d1285064","colab":{"base_uri":"https://localhost:8080/"},"id":"k7DJrepEBqTO","executionInfo":{"status":"ok","timestamp":1650276706294,"user_tz":-330,"elapsed":38978,"user":{"displayName":"JaniceMarian Jockim","userId":"17707824950601049259"}},"outputId":"e48e77f1-26a0-4dfb-cebf-eea5eb25769f"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.837657912669707"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["It's most likely possible to achieve a higher score with more tuning, or a more advanced approach. Lets check on how it does on a couple of sentences."],"metadata":{"_cell_guid":"6f1e98e8-f816-5e9a-f0f9-957798dfa8c1","id":"BaH7FwxlBqTP"}},{"cell_type":"code","source":["sentences = count_vectorizer.transform([\n","    \"What a great airline, the trip was a pleasure!\",\n","    \"My issue was quickly resolved after calling customer support. Thanks!\",\n","    \"What the hell! My flight was cancelled again. This sucks!\",\n","    \"Service was awful. I'll never fly with you again.\",\n","    \"You fuckers lost my luggage. Never again!\",\n","    \"I have mixed feelings about airlines. I don't know what I think.\",\n","    \"\"\n","])\n","clf.predict_proba(sentences)"],"metadata":{"_cell_guid":"c8a2059d-c320-c958-613e-0085c791f128","id":"Lao6NQevBqTP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So while results aren't very impressive overall, we can see that it's doing a good job on these obvious sentences. \n","\n","## What is hard for the classifier?\n","\n","It's interesting to know which sentences are hard. To find out, lets apply the classifier to all our test sentences and sort by the marginal probability."],"metadata":{"_cell_guid":"74935e17-0463-78c9-c8be-e2ba71fdfe21","id":"BosLQD2VBqTQ"}},{"cell_type":"markdown","source":["Here are some of the hardest sentences."],"metadata":{"_cell_guid":"179f152a-dd8b-13ac-3b3a-649b16f0bcf2","id":"crVNXzHGBqTQ"}},{"cell_type":"code","source":["predictions_on_test_data = clf.predict_proba(data_test)\n","index = np.transpose(np.array([range(0,len(predictions_on_test_data))]))\n","indexed_predictions = np.concatenate((predictions_on_test_data, index), axis=1).tolist()"],"metadata":{"_cell_guid":"c94ef2e0-bd5d-bfb9-16ac-90459b0d29e0","id":"7ae7hk-dBqTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def marginal(p):\n","    top2 = p.argsort()[::-1]\n","    return abs(p[top2[0]]-p[top2[1]])\n","margin = sorted(list(map(lambda p : [marginal(np.array(p[0:3])),p[3]], indexed_predictions)), key=lambda p : p[0])\n","list(map(lambda p : tweets.iloc[data_test_index[int(p[1])].toarray()[0][0]].text, margin[0:10]))"],"metadata":{"_cell_guid":"a1346d01-0e5d-8f01-b364-1523c8665203","id":"h5DzSoRABqTR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["and their probability distributions?"],"metadata":{"_cell_guid":"9d28561f-5767-29f8-a49c-85e191b63c4b","id":"nHuWcmiDBqTS"}},{"cell_type":"code","source":["list(map(lambda p : predictions_on_test_data[int(p[1])], margin[0:10]))"],"metadata":{"_cell_guid":"fa092618-003a-c2e5-1228-a95aa18337ef","id":"FWhWaH6vBqTS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How about the easiest sentences?"],"metadata":{"_cell_guid":"3d8c9fdd-8e68-0fd1-3f06-bfaaeba4b79f","id":"jPCk0r73BqTT"}},{"cell_type":"code","source":["list(map(lambda p : tweets.iloc[data_test_index[int(p[1])].toarray()[0][0]].text, margin[-10:]))"],"metadata":{"_cell_guid":"c2468ac1-b7fb-889c-50a3-e5d7af0269d5","id":"GIEjc8jpBqTT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["and their probability distributions?"],"metadata":{"_cell_guid":"d2387867-ac36-8219-0d71-fb0f578d4913","id":"R0_hpDxHBqTU"}},{"cell_type":"code","source":["list(map(lambda p : predictions_on_test_data[int(p[1])], margin[-10:]))"],"metadata":{"_cell_guid":"80f2aff9-3273-caab-b9a4-959aebc9d69d","id":"Nh4zUcB3BqTU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looks like all of the easiest sentences are negative. What is the distribution of certainty across all sentences?"],"metadata":{"_cell_guid":"254f79d4-feee-706a-a2a0-a602069be3cc","id":"mxEuJOVsBqTU"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","marginal_probs = list(map(lambda p : p[0], margin))\n","n, bins, patches = plt.hist(marginal_probs, 25, facecolor='blue', alpha=0.75)\n","plt.title('Marginal confidence histogram - All data')\n","plt.ylabel('Count')\n","plt.xlabel('Marginal confidence')\n","plt.show()"],"metadata":{"_cell_guid":"48fe3201-eb49-9617-eeb5-fca45beb657f","id":"ge5QxZV9BqTV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets break it down by positive and negative sentiment to see if one is harder than the other.\n","\n","### Positive data"],"metadata":{"_cell_guid":"c3e7841f-d705-d9bc-3dc9-e2117cea3b26","id":"picsSwBpBqTV"}},{"cell_type":"code","source":["positive_test_data = list(filter(lambda row : row[0]==2, hstack((targets_test[:,None], data_test)).toarray()))\n","positive_probs = clf.predict_proba(list(map(lambda r : r[1:], positive_test_data)))\n","marginal_positive_probs = list(map(lambda p : marginal(p), positive_probs))\n","n, bins, patches = plt.hist(marginal_positive_probs, 25, facecolor='green', alpha=0.75)\n","plt.title('Marginal confidence histogram - Positive data')\n","plt.ylabel('Count')\n","plt.xlabel('Marginal confidence')\n","plt.show()"],"metadata":{"_cell_guid":"1709f0eb-febe-0161-f1df-ac9585ff580d","id":"AuSiQs-TBqTW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Neutral data"],"metadata":{"_cell_guid":"98258e2a-fbd7-9e18-b43c-1f2330124e67","id":"c2szFSVQBqTW"}},{"cell_type":"code","source":["positive_test_data = list(filter(lambda row : row[0]==1, hstack((targets_test[:,None], data_test)).toarray()))\n","positive_probs = clf.predict_proba(list(map(lambda r : r[1:], positive_test_data)))\n","marginal_positive_probs = list(map(lambda p : marginal(p), positive_probs))\n","n, bins, patches = plt.hist(marginal_positive_probs, 25, facecolor='blue', alpha=0.75)\n","plt.title('Marginal confidence histogram - Neutral data')\n","plt.ylabel('Count')\n","plt.xlabel('Marginal confidence')\n","plt.show()"],"metadata":{"_cell_guid":"eea28df4-f4a2-0b9b-2547-173935fe551c","id":"ZKe9Bj5yBqTX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Negative data"],"metadata":{"_cell_guid":"c43fb9ec-8751-f1e6-24a0-b77f18dacefd","id":"ZZiWpxteBqTX"}},{"cell_type":"code","source":["negative_test_data = list(filter(lambda row : row[0]==0, hstack((targets_test[:,None], data_test)).toarray()))\n","negative_probs = clf.predict_proba(list(map(lambda r : r[1:], negative_test_data)))\n","marginal_negative_probs = list(map(lambda p : marginal(p), negative_probs))\n","n, bins, patches = plt.hist(marginal_negative_probs, 25, facecolor='red', alpha=0.75)\n","plt.title('Marginal confidence histogram - Negative data')\n","plt.ylabel('Count')\n","plt.xlabel('Marginal confidence')\n","plt.show()"],"metadata":{"_cell_guid":"52fda712-b5c7-7bb2-0333-68a888547226","id":"pFc9RYGgBqTY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clearly the positive data is much harder for the classifier. This makes sense since there's a lot less of it. An important challenge in building a better classifier will then be how to handle positive data.\n","\n","A more advanced classifier is described here."],"metadata":{"_cell_guid":"90217d44-58a8-782f-5b68-9ee7032aa9c7","id":"MowPI0X7BqTY"}}]}
